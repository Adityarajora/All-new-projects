{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff0a3a0",
   "metadata": {},
   "source": [
    "# In this notebook, I'll apply Encode-decoder architecture with attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b5112",
   "metadata": {},
   "source": [
    "## Overview\n",
    "***\n",
    "*A chatbot or chatterbot is a software application used to conduct an on-line chat conversation via text or text-to-speech, in lieu of providing direct contact with a live human agent chatbot is a type of software that can help human by automating conversations and interact with them through messaging platforms. here are different approaches and tools that you can use when building chatbots. Depending on the use case you want to address, some technologies are more appropriate than others. Combining artificial intelligence forms such as natural language processing, machine learning, and semantic understanding may be the best option to achieve the desired results.*\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bcc417",
   "metadata": {},
   "source": [
    "## How to build a Chatbot for our task?\n",
    "***\n",
    "ChatBots are usually Task specific means if there a chatbot which serves only food delivery app have trained on a dataset which\n",
    "completely different from the dataset on which chatbot which serves online healthcare app. Similary, for this kaggle problem\n",
    "we have provided with movie dataset which may feel that its not specific to any task, but actually it is specific to how people\n",
    "will interect generally as these movie dialogues are nothing but daily life conversation between people however, that chatbot\n",
    "may reply things which sounds too much dramatic and filmy like some dialogue of Tom cruise, shah rukh khan etc.\n",
    "\n",
    "We can approch this problem by applying Neural network models like encoder-decoder architecture with some attention mechanism.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a2e9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import ast\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127a1155",
   "metadata": {},
   "source": [
    "## Loading data, preprared while EDA and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c375b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "train = joblib.load(\"train\")\n",
    "validation = joblib.load(\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d1df2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tknizer_q = Tokenizer(filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "tknizer_q.fit_on_texts(train['question'].values)\n",
    "tknizer_a = Tokenizer(filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "tknizer_a.fit_on_texts(train['answer_in'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4627ba4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29704\n",
      "29595\n"
     ]
    }
   ],
   "source": [
    "q_word_idx = tknizer_q.word_index\n",
    "q_idx_word = {v: k for k, v in q_word_idx.items()}\n",
    "\n",
    "a_word_idx = tknizer_a.word_index\n",
    "a_idx_word = {v: k for k, v in a_word_idx.items()}\n",
    "\n",
    "print(len(q_word_idx.keys()))\n",
    "print(len(a_word_idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd5c82ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<start> i am glad they let you out <end>',\n",
       "       '<start> some i have been talking to the parkers one thing i know i am not doing  i am not going back',\n",
       "       '<start> do not i get a kiss', ...,\n",
       "       '<start> oh sure you are biased  you are a fellow muncian but would an imbecile come up with this',\n",
       "       '<start> dr crowe',\n",
       "       '<start> nothing you got a hell of a way to make a living'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['answer_in'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04070324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14920)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_word_idx['<start>'], a_word_idx['<end>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f319c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_a = len(a_word_idx.keys())\n",
    "vocab_size_q = len(q_word_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "808e7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data, tknizer_q, tknizer_a):\n",
    "        self.encoder_inps = data['question'].values\n",
    "        self.decoder_inps = data['answer_in'].values\n",
    "        self.decoder_outs = data['answer_out'].values\n",
    "        self.tknizer_q = tknizer_q\n",
    "        self.tknizer_a = tknizer_a\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        self.encoder_seq = self.tknizer_q.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
    "        self.decoder_inp_seq = self.tknizer_a.texts_to_sequences([self.decoder_inps[i]])\n",
    "        self.decoder_out_seq = self.tknizer_a.texts_to_sequences([self.decoder_outs[i]])\n",
    "\n",
    "        self.encoder_seq = pad_sequences(self.encoder_seq,maxlen=27, dtype='int32', padding='post')\n",
    "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq,maxlen=27, dtype='int32', padding='post')\n",
    "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq,maxlen=27, dtype='int32', padding='post')\n",
    "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
    "\n",
    "    def __len__(self): # your model.fit_gen requires this function\n",
    "        return len(self.encoder_inps)\n",
    "\n",
    "    \n",
    "class Dataloder(tf.keras.utils.Sequence):    \n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "\n",
    "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
    "        return tuple([[batch[0],batch[1]],batch[2]])\n",
    "\n",
    "    def __len__(self):  # your model.fit_gen requires this function\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3403777d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 27) (128, 27) (128, 27)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(train, tknizer_q, tknizer_a)\n",
    "test_dataset  = Dataset(validation, tknizer_q, tknizer_a)\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=128)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=128)\n",
    "\n",
    "print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211926d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ([q,a_in],a_out), ([q,a_in],a_out), ([q,a_in],a_out) ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5726d8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "599aea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "    def __init__(self, ques_vocab_size, embedding_dim, encoder_input_length, lstm_units):\n",
    "\n",
    "        #Initialize Embedding layer\n",
    "        #Intialize Encoder LSTM layer\n",
    "        super().__init__(name=\"encode_model_attention\")\n",
    "        self.ques_vocab_size = ques_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.encoder_input_length = encoder_input_length\n",
    "        self.embedding = Embedding(input_dim=self.ques_vocab_size, output_dim=self.embedding_dim, input_length=self.encoder_input_length,\\\n",
    "                  mask_zero=True, name=\"embedding_layer_encoder\")\n",
    "        self.LSTM = LSTM(self.lstm_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
    "\n",
    "    def call(self,input_sequence):\n",
    "        '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "          returns -- All encoder_outputs, last time steps hidden and cell state\n",
    "        '''\n",
    "        input_embeddings = self.embedding(input_sequence)\n",
    "        self.lstm_output, self.lstm_state_h, self.lstm_state_c = self.LSTM(input_embeddings)\n",
    "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
    "\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "        '''\n",
    "        Given a batch size it will return intial hidden state and intial cell state.\n",
    "        If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "        '''\n",
    "        lstm_state_h = tf.zeros([batch_size, self.lstm_size])\n",
    "        lstm_state_c = tf.zeros([batch_size, self.lstm_size])\n",
    "\n",
    "        return lstm_state_h, lstm_state_c\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42a459d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
    "    '''\n",
    "    def __init__(self, att_units):\n",
    "\n",
    "        super().__init__(name=\"Attention_Bahdanau\")\n",
    "        self.K = 100\n",
    "        self.dense_1 = Dense(self.K, activation='relu')\n",
    "        self.dense_2 = Dense(self.K, activation='relu')\n",
    "        self.dense_3 = Dense(1, activation='relu')\n",
    "  \n",
    "\n",
    "    def call(self, decoder_hidden_state, encoder_output):\n",
    "        '''\n",
    "          Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
    "          * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
    "            Multiply the score function with your encoder_outputs to get the context vector.\n",
    "            Function returns context vector and attention weights(softmax - scores)\n",
    "        '''\n",
    "            \n",
    "        k1 = self.dense_1(encoder_output)\n",
    "        k2 =  self.dense_2(decoder_hidden_state)\n",
    "        add = tf.keras.layers.Add()([k1, k2])\n",
    "        tanh = tf.keras.layers.Activation(activation=\"tanh\")(add)\n",
    "        ei = self.dense_3(tanh)\n",
    "        ei = tf.squeeze(ei,-1)\n",
    "        alphas = Softmax()(ei) \n",
    "        alphas = tf.expand_dims(alphas, axis=-1)\n",
    "        mull = tf.keras.layers.Multiply()([encoder_output, alphas])\n",
    "        context_vec = tf.reduce_mean(mull, axis=-2)                \n",
    "        return context_vec, alphas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff4c1571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepDecoder(tf.keras.Model):\n",
    "    def __init__(self,ans_vocab_size, embedding_dim, decoder_input_length, lstm_units, attention_units):\n",
    "        super().__init__(name=\"OneStepDecoder\")\n",
    "        # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "        \n",
    "        self.lstm_units = lstm_units\n",
    "        self.attention_units = attention_units\n",
    "        self.attention = Attention(attention_units)\n",
    "        self.embedding = Embedding(input_dim = ans_vocab_size, output_dim = embedding_dim, input_length = decoder_input_length,\n",
    "                                   mask_zero=True, trainable=True )\n",
    "        #weights=[embedding_matrix]\n",
    "        \n",
    "        self.lstm = LSTM(self.lstm_units, return_sequences=True, return_state=True,name=\"OneStepDecoder_LSTM\")\n",
    "        self.dense = Dense(ans_vocab_size, activation='softmax')\n",
    "        \n",
    "\n",
    "    def call(self, input_to_decoder, encoder_output, state_h, state_c):\n",
    "        '''\n",
    "             One step decoder mechanisim step by step:\n",
    "          A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "          B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "          C. Concat the context vector with the step A output\n",
    "          D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "          E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "          F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "        '''\n",
    "        #adding attention information to embeddings now\n",
    "        embeddings = self.embedding(input_to_decoder)\n",
    "        context_vector, attention_weights = self.attention(state_h, encoder_output)\n",
    "        attention_embeddings = tf.concat([tf.expand_dims(context_vector, 1), embeddings], axis=-1)\n",
    "        \n",
    "        x, state_h, state_c = self.lstm(attention_embeddings, initial_state=[state_h, state_c])\n",
    "        \n",
    "        output = self.dense(x)\n",
    "        \n",
    "        output = tf.squeeze(output,1)\n",
    "        \n",
    "        return output, state_h, state_c, attention_weights, context_vector\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78e820da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, ans_vocab_size, embedding_dim, decoder_input_length, lstm_units, attention_units):\n",
    "        #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "        super().__init__(name=\"Decode_Attention\")\n",
    "        self.ans_vocab_size = ans_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.decoder_input_length = decoder_input_length\n",
    "        self.lstm_units = lstm_units\n",
    "        self.attention_units = attention_units\n",
    "        self.onestepdecoder = OneStepDecoder(self.ans_vocab_size, \\\n",
    "                                             self.embedding_dim, \\\n",
    "                                             self.decoder_input_length, \\\n",
    "                                             self.lstm_units, self.attention_units)\n",
    "        \n",
    "    def call(self, input_to_decoder, encoder_output, decoder_hidden_state, decoder_cell_state):\n",
    "\n",
    "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
    "        #Create a tensor array as shown in the reference notebook\n",
    "        \n",
    "        #Iterate till the length of the decoder input\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            # Store the output in tensorarray\n",
    "        # Return the tensor array\n",
    "        \n",
    "        all_outputs = tf.TensorArray(tf.float32, size=27, name=\"output_arrays\")\n",
    "        for timestep in range(27):\n",
    "            output,decoder_hidden_state,decoder_cell_state,attention_weights,context_vector = self.onestepdecoder(\\\n",
    "                                input_to_decoder[:, timestep:timestep+1],encoder_output,\\\n",
    "                                decoder_hidden_state,decoder_cell_state\n",
    "                               )\n",
    "            \n",
    "            all_outputs = all_outputs.write(timestep, output)\n",
    "        all_outputs = tf.transpose(all_outputs.stack(), [1,0,2])\n",
    "        return all_outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d387f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "    def __init__(self,ques_vocab_size, ans_vocab_size, encoder_input_length, dencoder_input_length, emb_dim, lstm_units, attention_units):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super().__init__(name=\"Encoder_Decoder_Attention_model\")\n",
    "    \n",
    "        self.encoder = Encoder(ques_vocab_size, emb_dim, encoder_input_length, lstm_units )\n",
    "        self.decoder = Decoder(ans_vocab_size, emb_dim, dencoder_input_length, lstm_units, attention_units)\n",
    "                             \n",
    "    def call(self,data):\n",
    "        #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "        # Decoder initial states are encoder final states, Initialize it accordingly\n",
    "        # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
    "        # return the decoder output\n",
    "        input_q, input_a = data[0], data[1]\n",
    "        encoder_output, encoder_h, encoder_c = self.encoder(input_q)\n",
    "        decoder_output = self.decoder(input_a, encoder_output, encoder_h, encoder_c)\n",
    "        return decoder_output\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab8f74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, TensorBoard, LearningRateScheduler\n",
    "from sklearn.metrics import recall_score, f1_score, roc_curve, auc\n",
    "import datetime\n",
    "\n",
    "filepath=\"model_save_attention_4/weights-{epoch:02d}-{val_loss:.2f}\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', save_format=\"tf\", save_freq=\"epoch\",  verbose=1, save_best_only=True, mode='auto')\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "1025facd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 2.1390 - accuracy: 0.7089\n",
      "Epoch 1: val_loss improved from inf to 1.70939, saving model to model_save_attention\\weights-01-1.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, dense_12_layer_call_fn, dense_12_layer_call_and_return_conditional_losses, dense_13_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention\\weights-01-1.71\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention\\weights-01-1.71\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000024090E417F0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x0000024090E6D400> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000240FB4D6A30> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 470s 504ms/step - loss: 2.1390 - accuracy: 0.7089 - val_loss: 1.7094 - val_accuracy: 0.7322\n",
      "Epoch 2/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.6838 - accuracy: 0.7389\n",
      "Epoch 2: val_loss improved from 1.70939 to 1.56630, saving model to model_save_attention\\weights-02-1.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, dense_12_layer_call_fn, dense_12_layer_call_and_return_conditional_losses, dense_13_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention\\weights-02-1.57\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention\\weights-02-1.57\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000024090E417F0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x0000024090E6D400> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000240FB4D6A30> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 436s 500ms/step - loss: 1.6838 - accuracy: 0.7389 - val_loss: 1.5663 - val_accuracy: 0.7480\n",
      "Epoch 3/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.5756 - accuracy: 0.7456\n",
      "Epoch 3: val_loss improved from 1.56630 to 1.51757, saving model to model_save_attention\\weights-03-1.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, dense_12_layer_call_fn, dense_12_layer_call_and_return_conditional_losses, dense_13_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention\\weights-03-1.52\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention\\weights-03-1.52\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000024090E417F0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x0000024090E6D400> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000240FB4D6A30> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 440s 505ms/step - loss: 1.5756 - accuracy: 0.7456 - val_loss: 1.5176 - val_accuracy: 0.7518\n",
      "Epoch 4/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.5113 - accuracy: 0.7493\n",
      "Epoch 4: val_loss improved from 1.51757 to 1.49310, saving model to model_save_attention\\weights-04-1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, dense_12_layer_call_fn, dense_12_layer_call_and_return_conditional_losses, dense_13_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention\\weights-04-1.49\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention\\weights-04-1.49\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000024090E417F0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x0000024090E6D400> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000240FB4D6A30> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 436s 501ms/step - loss: 1.5113 - accuracy: 0.7493 - val_loss: 1.4931 - val_accuracy: 0.7537\n",
      "Epoch 5/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.4597 - accuracy: 0.7520\n",
      "Epoch 5: val_loss improved from 1.49310 to 1.47880, saving model to model_save_attention\\weights-05-1.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, dense_12_layer_call_fn, dense_12_layer_call_and_return_conditional_losses, dense_13_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention\\weights-05-1.48\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention\\weights-05-1.48\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000024090E417F0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x0000024090E6D400> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000240FB4D6A30> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 436s 501ms/step - loss: 1.4597 - accuracy: 0.7520 - val_loss: 1.4788 - val_accuracy: 0.7550\n",
      "Epoch 6/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.4136 - accuracy: 0.7543\n",
      "Epoch 6: val_loss improved from 1.47880 to 1.47275, saving model to model_save_attention\\weights-06-1.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, dense_12_layer_call_fn, dense_12_layer_call_and_return_conditional_losses, dense_13_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention\\weights-06-1.47\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention\\weights-06-1.47\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000024090E417F0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x0000024090E6D400> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000240FB4D6A30> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 435s 500ms/step - loss: 1.4136 - accuracy: 0.7543 - val_loss: 1.4727 - val_accuracy: 0.7562\n",
      "Epoch 7/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.3701 - accuracy: 0.7565\n",
      "Epoch 7: val_loss improved from 1.47275 to 1.46967, saving model to model_save_attention\\weights-07-1.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, dense_12_layer_call_fn, dense_12_layer_call_and_return_conditional_losses, dense_13_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention\\weights-07-1.47\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention\\weights-07-1.47\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000024090E417F0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x0000024090E6D400> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000240FB4D6A30> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 441s 506ms/step - loss: 1.3701 - accuracy: 0.7565 - val_loss: 1.4697 - val_accuracy: 0.7566\n",
      "Epoch 8/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.3279 - accuracy: 0.7587\n",
      "Epoch 8: val_loss did not improve from 1.46967\n",
      "871/871 [==============================] - 395s 454ms/step - loss: 1.3279 - accuracy: 0.7587 - val_loss: 1.4721 - val_accuracy: 0.7569\n",
      "Epoch 9/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.2868 - accuracy: 0.7614\n",
      "Epoch 9: val_loss did not improve from 1.46967\n",
      "871/871 [==============================] - 380s 437ms/step - loss: 1.2868 - accuracy: 0.7614 - val_loss: 1.4764 - val_accuracy: 0.7570\n",
      "Epoch 10/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.2465 - accuracy: 0.7648\n",
      "Epoch 10: val_loss did not improve from 1.46967\n",
      "871/871 [==============================] - 376s 431ms/step - loss: 1.2465 - accuracy: 0.7648 - val_loss: 1.4839 - val_accuracy: 0.7566\n",
      "Epoch 11/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.2083 - accuracy: 0.7688\n",
      "Epoch 11: val_loss did not improve from 1.46967\n",
      "871/871 [==============================] - 379s 435ms/step - loss: 1.2083 - accuracy: 0.7688 - val_loss: 1.4919 - val_accuracy: 0.7570\n",
      "Epoch 12/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.1723 - accuracy: 0.7727\n",
      "Epoch 12: val_loss did not improve from 1.46967\n",
      "871/871 [==============================] - 377s 433ms/step - loss: 1.1723 - accuracy: 0.7727 - val_loss: 1.4994 - val_accuracy: 0.7563\n",
      "Epoch 13/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.1389 - accuracy: 0.7765\n",
      "Epoch 13: val_loss did not improve from 1.46967\n",
      "871/871 [==============================] - 387s 444ms/step - loss: 1.1389 - accuracy: 0.7765 - val_loss: 1.5081 - val_accuracy: 0.7562\n",
      "Epoch 14/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.1080 - accuracy: 0.7804\n",
      "Epoch 14: val_loss did not improve from 1.46967\n",
      "871/871 [==============================] - 373s 428ms/step - loss: 1.1080 - accuracy: 0.7804 - val_loss: 1.5187 - val_accuracy: 0.7556\n",
      "Epoch 15/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.0787 - accuracy: 0.7843\n",
      "Epoch 15: val_loss did not improve from 1.46967\n",
      "871/871 [==============================] - 376s 431ms/step - loss: 1.0787 - accuracy: 0.7843 - val_loss: 1.5283 - val_accuracy: 0.7551\n",
      "Epoch 16/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.0509 - accuracy: 0.7880\n",
      "Epoch 16: val_loss did not improve from 1.46967\n",
      "871/871 [==============================] - 384s 441ms/step - loss: 1.0509 - accuracy: 0.7880 - val_loss: 1.5393 - val_accuracy: 0.7542\n",
      "Epoch 17/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.0246 - accuracy: 0.7918\n",
      "Epoch 17: val_loss did not improve from 1.46967\n",
      "871/871 [==============================] - 383s 440ms/step - loss: 1.0246 - accuracy: 0.7918 - val_loss: 1.5500 - val_accuracy: 0.7544\n",
      "Epoch 18/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 0.9994 - accuracy: 0.7954\n",
      "Epoch 18: val_loss did not improve from 1.46967\n",
      "871/871 [==============================] - 376s 432ms/step - loss: 0.9994 - accuracy: 0.7954 - val_loss: 1.5612 - val_accuracy: 0.7535\n",
      "Epoch 19/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 0.9752 - accuracy: 0.7989\n",
      "Epoch 19: val_loss did not improve from 1.46967\n",
      "871/871 [==============================] - 377s 433ms/step - loss: 0.9752 - accuracy: 0.7989 - val_loss: 1.5723 - val_accuracy: 0.7531\n",
      "Epoch 20/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 0.9519 - accuracy: 0.8026\n",
      "Epoch 20: val_loss did not improve from 1.46967\n",
      "871/871 [==============================] - 379s 435ms/step - loss: 0.9519 - accuracy: 0.8026 - val_loss: 1.5845 - val_accuracy: 0.7525\n"
     ]
    }
   ],
   "source": [
    "ques_vocab_size = vocab_size_q + 1\n",
    "ans_vocab_size = vocab_size_a + 1\n",
    "encoder_input_length = 27\n",
    "dencoder_input_length = 27\n",
    "lstm_units = 512\n",
    "attention_units = 512\n",
    "emb_dim = 100\n",
    "\n",
    "model_attention = encoder_decoder(ques_vocab_size, \n",
    "                                  ans_vocab_size, \n",
    "                                  encoder_input_length, \n",
    "                                  dencoder_input_length,\n",
    "                                  emb_dim,\n",
    "                                  lstm_units, \n",
    "                                  attention_units)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model_attention.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "train_steps=train.shape[0]//128\n",
    "valid_steps=validation.shape[0]//128\n",
    "history_dot = model_attention.fit(train_dataloader, \\\n",
    "                                      steps_per_epoch=train_steps, \\\n",
    "                                      epochs=20, \\\n",
    "                                      validation_data=test_dataloader, \\\n",
    "                                      validation_steps=valid_steps,\n",
    "                                      callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "95bf8f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x242d54021f0>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history_dot.history['loss']\n",
    "val_loss = history_dot.history['val_loss']\n",
    "epoch = 20\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss/val_loss')\n",
    "plt.title('Learning')\n",
    "plt.plot(range(epoch), loss, label = \"loss\")\n",
    "plt.plot(range(epoch), val_loss, label = \"val_loss\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "b0f8f451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder_Decoder_Attention_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encode_model_attention (Enc  multiple                 4215024   \n",
      " oder)                                                           \n",
      "                                                                 \n",
      " Decode_Attention (Decoder)  multiple                  20523303  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,738,327\n",
      "Trainable params: 24,738,327\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_attention.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d285fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    sentence = sentence.split(\" \")\n",
    "    predicted_sentence = predicted_sentence.split(\" \")\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50905e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_test_sentence):\n",
    "\n",
    "    '''\n",
    "    A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
    "    B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "    C. Initialize index of <start> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
    "    D. till we reach max_length of decoder or till the model predicted word <end>:\n",
    "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
    "         Save the attention weights\n",
    "         And get the word using the tokenizer(word index) and then store it in a string.\n",
    "    E. Call plot_attention(#params)\n",
    "    F. Return the predicted sentence\n",
    "    '''\n",
    "    ENCODER_SEQ_LEN = 27\n",
    "    DECODER_SEQ_LEN = 27 \n",
    "    nums = tknizer_q.texts_to_sequences([input_test_sentence])\n",
    "    nums_padded = pad_sequences(nums, maxlen=27, dtype='int32', padding='post')\n",
    "    encoder_output, enc_state_h, enc_state_c = model_attention.layers[0](nums_padded)\n",
    "    pred, alphas = [], []\n",
    "    cur_vec = np.ones((1, 1))*1\n",
    "    osd = model_attention.layers[1].onestepdecoder\n",
    "    for i in range(DECODER_SEQ_LEN):\n",
    "        \n",
    "        \n",
    "        output, state_h, state_c, attention_weights, context_vector = osd(cur_vec, encoder_output, enc_state_h, enc_state_c )\n",
    "        \n",
    "        enc_state_h, enc_state_c = state_h, state_c\n",
    "        alphas.append(attention_weights.numpy().flatten())\n",
    "        \n",
    " \n",
    "        cur_vec = np.reshape(np.argmax(output), (1, 1)) \n",
    "#         print(f\"at time step {i} the word is \", cur_vec)\n",
    "        \n",
    "        if a_idx_word[cur_vec[0][0]] == '<end>':\n",
    "            break\n",
    "        pred.append(cur_vec)\n",
    "        \n",
    "        \n",
    "     \n",
    "    pred_string = \"\"\n",
    "\n",
    "    pred_string = \" \".join([a_idx_word[i[0][0]] for i in pred]) + \" <end>\"\n",
    "    \n",
    "#     print(\"PREDICTED STRING:\",pred_string)\n",
    "    \n",
    "   \n",
    "    \n",
    "#     plot_attention(alphas, input_test_sentence, pred_string)\n",
    "    \n",
    "    return  input_test_sentence, pred_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "be7515eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4.3833587e-08 1.2463202e-08 1.8376322e-01 ... 1.0806967e-08\n",
      "  1.1699211e-08 1.2061286e-08]], shape=(1, 29554), dtype=float32)\n",
      "at time step 0 the word is  [[3]]\n",
      "tf.Tensor(\n",
      "[[2.5590246e-08 8.2964409e-09 2.6101186e-03 ... 8.6477243e-09\n",
      "  8.9658876e-09 8.1512415e-09]], shape=(1, 29554), dtype=float32)\n",
      "at time step 1 the word is  [[13]]\n",
      "tf.Tensor(\n",
      "[[3.3584974e-10 4.0732671e-09 4.3675222e-04 ... 4.2360124e-09\n",
      "  4.2827257e-09 3.7426910e-09]], shape=(1, 29554), dtype=float32)\n",
      "at time step 2 the word is  [[9]]\n",
      "tf.Tensor(\n",
      "[[3.5733260e-07 4.2503455e-08 5.6771314e-06 ... 4.0967659e-08\n",
      "  4.1249145e-08 4.1009759e-08]], shape=(1, 29554), dtype=float32)\n",
      "at time step 3 the word is  [[13450]]\n",
      "tf.Tensor(\n",
      "[[1.8505514e-07 5.3202644e-08 7.4637961e-04 ... 5.9698792e-08\n",
      "  5.7581587e-08 5.6036765e-08]], shape=(1, 29554), dtype=float32)\n",
      "at time step 4 the word is  [[750]]\n",
      "tf.Tensor(\n",
      "[[4.9667236e-08 5.6355240e-09 1.4667844e-02 ... 5.3862936e-09\n",
      "  5.9934142e-09 6.0920251e-09]], shape=(1, 29554), dtype=float32)\n",
      "at time step 5 the word is  [[4417]]\n",
      "tf.Tensor(\n",
      "[[4.1801101e-03 1.3057548e-08 1.8776468e-03 ... 1.1953153e-08\n",
      "  1.3346462e-08 1.1908869e-08]], shape=(1, 29554), dtype=float32)\n",
      "at time step 6 the word is  [[196]]\n",
      "tf.Tensor(\n",
      "[[2.4018904e-05 8.7059000e-08 1.3825527e-02 ... 8.6581920e-08\n",
      "  9.2962665e-08 8.9607290e-08]], shape=(1, 29554), dtype=float32)\n",
      "at time step 7 the word is  [[14870]]\n",
      "PREDICTED STRING: you are a celebrated cop twentysix years <end>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_sentence: you did\n",
      "English predict: you are a celebrated cop twentysix years\n",
      "English actual: spare me your classical answer measurements in  results taken  what if i did not miss\n"
     ]
    }
   ],
   "source": [
    "index = 450\n",
    "input_test_sentence = validation[\"question\"].values[index]\n",
    "actual = validation[\"answer_out\"].values[index]\n",
    "input_sentence, pred_string = predict(input_test_sentence)\n",
    "import re\n",
    "print(f\"Input_sentence: {input_sentence}\")\n",
    "print(f\"English predict: {re.sub('<end>', '', pred_string).strip()}\")\n",
    "print(f\"English actual: {re.sub('<end>', '', actual).strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a38197",
   "metadata": {},
   "source": [
    "# Output is making some sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd712f",
   "metadata": {},
   "source": [
    "### Let's change the hyperparameter a bit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "194de8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 2.1666 - accuracy: 0.7080\n",
      "Epoch 1: val_loss improved from inf to 1.83630, saving model to model_save_attention_2\\weights-01-1.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-01-1.84\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-01-1.84\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247F2776370> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x000002468CB80460> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247FEBDF1C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 387s 409ms/step - loss: 2.1666 - accuracy: 0.7080 - val_loss: 1.8363 - val_accuracy: 0.7264\n",
      "Epoch 2/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.7279 - accuracy: 0.7357\n",
      "Epoch 2: val_loss improved from 1.83630 to 1.59923, saving model to model_save_attention_2\\weights-02-1.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-02-1.60\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-02-1.60\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247F2776370> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x000002468CB80460> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247FEBDF1C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 348s 399ms/step - loss: 1.7279 - accuracy: 0.7357 - val_loss: 1.5992 - val_accuracy: 0.7440\n",
      "Epoch 3/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.6027 - accuracy: 0.7440\n",
      "Epoch 3: val_loss improved from 1.59923 to 1.54597, saving model to model_save_attention_2\\weights-03-1.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-03-1.55\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-03-1.55\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247F2776370> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x000002468CB80460> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247FEBDF1C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 351s 403ms/step - loss: 1.6027 - accuracy: 0.7440 - val_loss: 1.5460 - val_accuracy: 0.7482\n",
      "Epoch 4/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.5396 - accuracy: 0.7476\n",
      "Epoch 4: val_loss improved from 1.54597 to 1.51970, saving model to model_save_attention_2\\weights-04-1.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-04-1.52\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-04-1.52\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247F2776370> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x000002468CB80460> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247FEBDF1C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 350s 402ms/step - loss: 1.5396 - accuracy: 0.7476 - val_loss: 1.5197 - val_accuracy: 0.7506\n",
      "Epoch 5/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.4921 - accuracy: 0.7503\n",
      "Epoch 5: val_loss improved from 1.51970 to 1.50271, saving model to model_save_attention_2\\weights-05-1.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-05-1.50\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-05-1.50\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247F2776370> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x000002468CB80460> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247FEBDF1C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 354s 407ms/step - loss: 1.4921 - accuracy: 0.7503 - val_loss: 1.5027 - val_accuracy: 0.7523\n",
      "Epoch 6/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.4507 - accuracy: 0.7523\n",
      "Epoch 6: val_loss improved from 1.50271 to 1.49320, saving model to model_save_attention_2\\weights-06-1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-06-1.49\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-06-1.49\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247F2776370> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x000002468CB80460> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247FEBDF1C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 355s 408ms/step - loss: 1.4507 - accuracy: 0.7523 - val_loss: 1.4932 - val_accuracy: 0.7532\n",
      "Epoch 7/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.4120 - accuracy: 0.7543\n",
      "Epoch 7: val_loss improved from 1.49320 to 1.48987, saving model to model_save_attention_2\\weights-07-1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-07-1.49\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-07-1.49\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247F2776370> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x000002468CB80460> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247FEBDF1C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 354s 407ms/step - loss: 1.4120 - accuracy: 0.7543 - val_loss: 1.4899 - val_accuracy: 0.7540\n",
      "Epoch 8/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.3746 - accuracy: 0.7560\n",
      "Epoch 8: val_loss improved from 1.48987 to 1.48837, saving model to model_save_attention_2\\weights-08-1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-08-1.49\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_2\\weights-08-1.49\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247F2776370> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x000002468CB80460> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000247FEBDF1C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 358s 411ms/step - loss: 1.3746 - accuracy: 0.7560 - val_loss: 1.4884 - val_accuracy: 0.7547\n",
      "Epoch 9/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.3379 - accuracy: 0.7578\n",
      "Epoch 9: val_loss did not improve from 1.48837\n",
      "871/871 [==============================] - 308s 354ms/step - loss: 1.3379 - accuracy: 0.7578 - val_loss: 1.4911 - val_accuracy: 0.7549\n",
      "Epoch 10/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.3018 - accuracy: 0.7601\n",
      "Epoch 10: val_loss did not improve from 1.48837\n",
      "871/871 [==============================] - 309s 355ms/step - loss: 1.3018 - accuracy: 0.7601 - val_loss: 1.4951 - val_accuracy: 0.7553\n"
     ]
    }
   ],
   "source": [
    "ques_vocab_size = vocab_size_q + 1\n",
    "ans_vocab_size = vocab_size_a + 1\n",
    "encoder_input_length = 27\n",
    "dencoder_input_length = 27\n",
    "lstm_units = 512\n",
    "attention_units = 512\n",
    "emb_dim = 50\n",
    "\n",
    "model_attention = encoder_decoder(ques_vocab_size, \n",
    "                                  ans_vocab_size, \n",
    "                                  encoder_input_length, \n",
    "                                  dencoder_input_length,\n",
    "                                  emb_dim,\n",
    "                                  lstm_units, \n",
    "                                  attention_units)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model_attention_2.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "train_steps=train.shape[0]//128\n",
    "valid_steps=validation.shape[0]//128\n",
    "history_dot = model_attention_2.fit(train_dataloader, \\\n",
    "                                      steps_per_epoch=train_steps, \\\n",
    "                                      epochs=10, \\\n",
    "                                      validation_data=test_dataloader, \\\n",
    "                                      validation_steps=valid_steps,\n",
    "                                      callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f3ef86bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder_Decoder_Attention_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encode_model_attention (Enc  multiple                 2631674   \n",
      " oder)                                                           \n",
      "                                                                 \n",
      " Decode_Attention (Decoder)  multiple                  18958404  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,590,078\n",
      "Trainable params: 21,590,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_attention_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c318d",
   "metadata": {},
   "source": [
    "## Not better than previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "823a41b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at time step 0 the word is  [[3]]\n",
      "at time step 1 the word is  [[13]]\n",
      "at time step 2 the word is  [[5]]\n",
      "at time step 3 the word is  [[66]]\n",
      "at time step 4 the word is  [[7]]\n",
      "at time step 5 the word is  [[30]]\n",
      "at time step 6 the word is  [[9]]\n",
      "at time step 7 the word is  [[114]]\n",
      "at time step 8 the word is  [[14920]]\n",
      "PREDICTED STRING: you are not going to be a little <end>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_sentence: in a magazine you come across a fullpage photo of a nude girl\n",
      "English predict: you are not going to be a little\n",
      "English actual: is this testing whether i am an android or a lesbian\n"
     ]
    }
   ],
   "source": [
    "model_attention = model_attention_2\n",
    "index = 13223\n",
    "\n",
    "input_test_sentence = validation[\"question\"].values[index]\n",
    "actual = validation[\"answer_out\"].values[index]\n",
    "input_sentence, pred_string = predict_concat(input_test_sentence)\n",
    "import re\n",
    "print(f\"Input_sentence: {input_sentence}\")\n",
    "print(f\"English predict: {re.sub('<end>', '', pred_string).strip()}\")\n",
    "print(f\"English actual: {re.sub('<end>', '', actual).strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a7e9e",
   "metadata": {},
   "source": [
    "## Increasing LSTM units, let's check!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1caeee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 2.1007 - accuracy: 0.7122\n",
      "Epoch 1: val_loss improved from inf to 1.65801, saving model to model_save_attention_4\\weights-01-1.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_4\\weights-01-1.66\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_4\\weights-01-1.66\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000026BF02ED7C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x0000026BF02F0A90> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000026BFC29E1F0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 433s 461ms/step - loss: 2.1007 - accuracy: 0.7122 - val_loss: 1.6580 - val_accuracy: 0.7418\n",
      "Epoch 2/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.6539 - accuracy: 0.7410\n",
      "Epoch 2: val_loss improved from 1.65801 to 1.53975, saving model to model_save_attention_4\\weights-02-1.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_4\\weights-02-1.54\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_4\\weights-02-1.54\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000026BF02ED7C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x0000026BF02F0A90> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000026BFC29E1F0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 391s 449ms/step - loss: 1.6539 - accuracy: 0.7410 - val_loss: 1.5398 - val_accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.5523 - accuracy: 0.7468\n",
      "Epoch 3: val_loss improved from 1.53975 to 1.49921, saving model to model_save_attention_4\\weights-03-1.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_4\\weights-03-1.50\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_4\\weights-03-1.50\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000026BF02ED7C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x0000026BF02F0A90> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000026BFC29E1F0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 388s 445ms/step - loss: 1.5523 - accuracy: 0.7468 - val_loss: 1.4992 - val_accuracy: 0.7534\n",
      "Epoch 4/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.4888 - accuracy: 0.7502\n",
      "Epoch 4: val_loss improved from 1.49921 to 1.47838, saving model to model_save_attention_4\\weights-04-1.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_4\\weights-04-1.48\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_4\\weights-04-1.48\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000026BF02ED7C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x0000026BF02F0A90> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000026BFC29E1F0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 390s 448ms/step - loss: 1.4888 - accuracy: 0.7502 - val_loss: 1.4784 - val_accuracy: 0.7551\n",
      "Epoch 5/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.4346 - accuracy: 0.7531\n",
      "Epoch 5: val_loss improved from 1.47838 to 1.46729, saving model to model_save_attention_4\\weights-05-1.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_4\\weights-05-1.47\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_4\\weights-05-1.47\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000026BF02ED7C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x0000026BF02F0A90> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000026BFC29E1F0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 392s 450ms/step - loss: 1.4346 - accuracy: 0.7531 - val_loss: 1.4673 - val_accuracy: 0.7563\n",
      "Epoch 6/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.3834 - accuracy: 0.7557\n",
      "Epoch 6: val_loss improved from 1.46729 to 1.46192, saving model to model_save_attention_4\\weights-06-1.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_4\\weights-06-1.46\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_attention_4\\weights-06-1.46\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000026BF02ED7C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.Attention object at 0x0000026BF02F0A90> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x0000026BFC29E1F0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 392s 450ms/step - loss: 1.3834 - accuracy: 0.7557 - val_loss: 1.4619 - val_accuracy: 0.7577\n",
      "Epoch 7/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.3338 - accuracy: 0.7582\n",
      "Epoch 7: val_loss did not improve from 1.46192\n",
      "871/871 [==============================] - 346s 397ms/step - loss: 1.3338 - accuracy: 0.7582 - val_loss: 1.4629 - val_accuracy: 0.7581\n",
      "Epoch 8/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.2854 - accuracy: 0.7614\n",
      "Epoch 8: val_loss did not improve from 1.46192\n",
      "871/871 [==============================] - 344s 395ms/step - loss: 1.2854 - accuracy: 0.7614 - val_loss: 1.4676 - val_accuracy: 0.7584\n",
      "Epoch 9/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.2380 - accuracy: 0.7654\n",
      "Epoch 9: val_loss did not improve from 1.46192\n",
      "871/871 [==============================] - 343s 394ms/step - loss: 1.2380 - accuracy: 0.7654 - val_loss: 1.4747 - val_accuracy: 0.7576\n",
      "Epoch 10/10\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.1930 - accuracy: 0.7700\n",
      "Epoch 10: val_loss did not improve from 1.46192\n",
      "871/871 [==============================] - 345s 396ms/step - loss: 1.1930 - accuracy: 0.7700 - val_loss: 1.4830 - val_accuracy: 0.7582\n"
     ]
    }
   ],
   "source": [
    "ques_vocab_size = vocab_size_q + 1\n",
    "ans_vocab_size = vocab_size_a + 1\n",
    "encoder_input_length = 27\n",
    "dencoder_input_length = 27\n",
    "lstm_units = 576\n",
    "attention_units = 576\n",
    "emb_dim = 100\n",
    "\n",
    "model_attention_3 = encoder_decoder(ques_vocab_size, \n",
    "                                  ans_vocab_size, \n",
    "                                  encoder_input_length, \n",
    "                                  dencoder_input_length,\n",
    "                                  emb_dim,\n",
    "                                  lstm_units, \n",
    "                                  attention_units)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model_attention_3.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "train_steps=train.shape[0]//128\n",
    "valid_steps=validation.shape[0]//128\n",
    "history_3 = model_attention_3.fit(train_dataloader, \\\n",
    "                                      steps_per_epoch=train_steps, \\\n",
    "                                      epochs=10, \\\n",
    "                                      validation_data=test_dataloader, \\\n",
    "                                      validation_steps=valid_steps,\n",
    "                                      callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "361dd15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder_Decoder_Attention_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encode_model_attention (Enc  multiple                 4530308   \n",
      " oder)                                                           \n",
      "                                                                 \n",
      " Decode_Attention (Decoder)  multiple                  23038905  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,569,213\n",
      "Trainable params: 27,569,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_attention_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a289aa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.1316193e-06 1.4866842e-08 2.1746123e-01 ... 1.3772574e-08\n",
      "  1.4987359e-08 1.4865537e-08]], shape=(1, 29596), dtype=float32)\n",
      "at time step 0 the word is  [[2]]\n",
      "tf.Tensor(\n",
      "[[1.9283561e-07 1.0089468e-08 5.5138487e-03 ... 9.8793098e-09\n",
      "  8.7075085e-09 1.0267940e-08]], shape=(1, 29596), dtype=float32)\n",
      "at time step 1 the word is  [[17]]\n",
      "tf.Tensor(\n",
      "[[6.6598068e-07 2.2859084e-08 6.3425386e-03 ... 2.1812706e-08\n",
      "  2.3563262e-08 2.2755900e-08]], shape=(1, 29596), dtype=float32)\n",
      "at time step 2 the word is  [[117]]\n",
      "tf.Tensor(\n",
      "[[2.2744962e-05 7.3077930e-09 1.3706234e-01 ... 5.9652661e-09\n",
      "  8.4261034e-09 7.9559452e-09]], shape=(1, 29596), dtype=float32)\n",
      "at time step 3 the word is  [[14863]]\n",
      "PREDICTED STRING: i am sorry <end>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAACRCAYAAAClvU7pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQ8ElEQVR4nO3debAlZXnH8e9vBpARGAiLgEQFFQTFlRFBECGYQDRaKXdcEQuMwT2oIUaJqZAIiiURExkX1AhquQJCNIoI7gREYUBBMBhQWWVfhmWe/NFnwvFyL9575iy3+34/VbfmnLe7f/0c+Oept9/uTlUhSZKkdls06QIkSZK05mzqJEmSOsCmTpIkqQNs6iRJkjrApk6SJKkDbOokSZI6wKZOkiSpA2zqJEmSOsCmTpIkqQPWmnQBC02SpwB7Aw9iSlNdVW+YSFGSJKn1bOrGKMkhwJHAJcBvgP53tPm+NkmSNLD47tfxSXI5cERVHTPpWiRJUre4pm68lgKnTroISZLUPTZ14/UZYN9JFyFJkrrHNXXjdTnw7iS7AecBd/VvrKr3T6QqSZLUeq6pG6Mk/3M/m6uqHj62YiRJUqfY1HVEks2Ba6pq1aRrkSRJ4+eaujFKsuWQ89ZOcmSSm4FfA1v3xo9I8tfDPJckSZrfbOrG69dJLkpybJL9htDkHQY8G3gZsLJv/Cxg/zXMliRJLWJTN17bAe8F1qN5CHF/k/fiAfL2A/6qqk4E+i+7ruidS5IkLRCuqZugJDsAb6OZaVtUVYvnePztwA5VdVnvEuzjq+qXSR4D/Kiq1h9+1ZIkaT7ykSZjlGQRsAzYC9gT2A24DjgeOH2AyAuAPYDLpoy/EDhn0DolSVL72NSN1w3AHcApwGdpLp3+ag3y3g18OslDgMXAC5JsD7wEeNYa1ipJklrENXXjdT7Nq8J2Bp4MLEuy6aBhVXUyzazcn9GsqTsM2BZ4dlV9c83LlSRJbeGaujFLsoTmsuuevb+dgF8Ap1fVG+eY9YiqunSGbXtX1WlrVq0kSWoLm7oJSbIFzdq6ZwEvYrAbJS4FdquqK6eMPwP4clVtMKx6JUnS/Obl1zFK8oIk/5bkZzQPCz6KZl3j64FHDxD5NeAbSTbqO8efAicCh6x5xZIkqS2cqRujJL8FzgC+DXy7qn6+hnkBTgAeCjwD2B34CvDmqlq+RsVKkqRWsalruSRrAScDfwTsSNPQfWSyVUmSpHGzqZtBkncB76uq26aMLwHeWlX/OGDuA4CX0lxuLeBC4ISqWnm/B957/JOmGV6P5ll3XwU+unqwqn48SI2SJKl9bOpmkOQeYMuqunrK+CbA1XO9qaF37KNp1sEtpXm8CcBjgRuBfavqZ7PIWEXTDKZvuP/76s81SI2SJKmdfPjwzELTIE31ROB3A2YeDZwLvLyqbgJIshT4NPABYJ9ZZGwz4LklSVKHOVM3Re8dqkVzSfM2fr+xWwysC3y4qg4eIPs24MlVdcGU8ccCP6yq9QYuXJIkLWjO1N3X62hm6T4OvIPm0uhqdwKXVdUPBsy+A9homvENe9vmrHejxM40d8Cu07+tqj41SKYkSWofZ+pmkOTpwPer6q4hZn6S5vVgBwI/7A3vChwLnFVVr5pj3vY0d75uQ9OI3kPTqN8FrKyqpUMqXZIkzXM+fLhPko37vp4PbJBk4+n+BjzFG2leCfYdmpm5O4AzgYuBNw+Q9wHgHJqZvtuAHYBlwE+A5w1YoyRJaiFn6vr03/Had5fpfXZjDe8sTfJImgYswIVVdcmAOdcBT6+qFUluBHauqot6s4wfrKrHDVqjJElqF9fU/b4/4d47W/caRmCSj/+BXf6yeTEEVNUBc42nmaEDuAbYCrgIuAJ45ByzJElSi9nU9amqM/q+XgPcU1UXwf+/U/WVwAXAkXOI3WzK9z2AVdz7nLodaS6DnzlAySuAxwO/BM4C3t6bbTwQGGj2T5IktZNN3cw+RvNcuYuS/DHNO1XPAA6meXjwobMJqapnr/6c5FDgduBVVXVrb2y93rnOnz7hfh1O8+gVgHfS3DRxOnAt8KIB8iRJUku5pm4GSW6gWaN2cZI3A8+pqr2S7AUcV1VbD5D5W2DvqrpwyvhjgNOqaosh1L0xcH35P1aSpAXFmbqZLaZ5Lh3A3sCpvc+XApsPmLk+8GCa97322xJ44GwCkpwEvKyqbup9nmk/gFtoLtF+qKpunGlfSZLUfjZ1M1sBvDbJV2mautWXW7eiubw5iC8CxyV5K/c+p24X4AjgS7PMuI5778q97g/s+wCa9XW7AM+ZW6mSJKlNvPw6gyR70Kyj2xD45Oo7U5P8C7BdVc35OXBJlgBHAQcAa/eG76ZZU3dIVd0207GDSvJo4L99BZkkSd1mU3c/kiwGllbV9X1jWwO3VdXVa5C7HvAImkeSXLL6polR6P2GHavqp6M6hyRJmjybOkmSpA7wNWGSJEkdYFMnSZLUATZ1s5TkoIWY2YYaR5HZhhpHkdmGGkeR2YYaR5HZhhpHkdmGGkeR2YYaR5HZhhqHlWlTN3tD/x/Yksw21DiKzDbUOIrMNtQ4isw21DiKzDbUOIrMNtQ4isw21DiKzDbUOJRMmzpJkqQOWPB3v66zaEktWbzBH9zvzlW3s86iJUM9dxsy21DjKDLbUOMoMttQ4ygy21DjKDLbUOMoMttQ4ygy21DjKDLbUONcMm+6+5prq2qz6bYt+DdKLFm8Abtu/PxJlyFJkvQHff3qf//VTNu8/CpJktQBNnWSJEkdYFMnSZLUAZ1t6pJ8IslXJ12HJEnSOHT5Rok3Apl0EZIkSePQ2aauqm6cdA2SJEnj4uVXSZKkDuhsUydJkrSQdPby6/3pvTT3IIB1F60/4WokSZLW3IKcqauq5VW1rKqWDfs1H5IkSZOwIJs6SZKkrrGpkyRJ6gCbOkmSpA6wqZMkSeqAzt79WlX7T7oGSZKkcXGmTpIkqQNs6iRJkjrApk6SJKkDOrumbvYKVt0z1MTLX739UPPu3KCGmgfw8PecN/RMSZI0Oc7USZIkdYBNnSRJUgfY1EmSJHWATZ0kSVIH2NRJkiR1gE2dJElSB9jUSZIkdcC8bOqS7JvkO0muT/K7JF9PskNv29ZJKsmLk5yR5PYk5yZ5XJIdk3w/ya1Jvptkm0n/FkmSpHGYl00dsB7wAWBnYE/gRuDkJOv07fNu4AjgicANwAnAB4F39I5bF/jXcRUsSZI0SfPyjRJV9cX+70leBdxE06xd0Rt+f1Wd2tt+FHAy8LyqOr03dgxwzNiKliRJmqB5OVOX5BFJTkhyaZKbgKtoan1o327977m6qvfv+VPG1kvywGnyD0pydpKz71x1x7DLlyRJGrt5OVNHM+v2a+A1vX/vBi4E+i+/3tX3ue5n7D6Na1UtB5YDbLj2ZsN/saokSdKYzbumLskmwA7AwX2XUp/EPKxVkiRpvpiPjdL1wLXAgUkuB7YC3kszWydJkqRpzLs1dVW1CngR8DhgBfAh4J3AyknWJUmSNJ/Nx5k6qupbwI5Thtfv+5wp+589zdjXpo5JkiR11bybqZMkSdLc2dRJkiR1gE2dJElSB8zLNXXjFVi0eLiRu94w1Lg3POo7Q80DOOldmw49U5IkTY4zdZIkSR1gUydJktQBNnWSJEkdYFMnSZLUATZ1kiRJHWBTJ0mS1AE2dZIkSR3QqqYuyTrTjC1KMuQHzUmSJLXLyJu6JHsk+WGSW5LcmORHSXbsbXtukvOTrExyeZJ3JEnfsZcl+YckH09yA3B8kv17Wc9MsgK4E9gtyV1Jtphy7sOTnDfq3yhJkjRpI23qkqwFnAh8F3g88BTgaOCeJDsBnwe+BDwW+FvgUOB1U2LeAvwcWAb8XW9sXeDvgdcAjwbOBS4FXtF37kW97x8bwU+TJEmaV0b9mrClwEbAyVV1aW/s5wBJjgfOqKrDeuMXJ9kWeDvwwb6MM6rqyNVfkuwGLAZeX1Xn9I1/FHg1sHrffYAHAZ+eWlSSg4CDANZdtP4a/kRJkqTJG+lMXVX9DvgE8PUkpyR5S5KH9DbvAHxvyiHfBbZKsrRv7Oxpou8GfjJl7JPAw5M8tff9AOArVXXdNHUtr6plVbVsnUVL5vSbJEmS5qORr6mrqlfRXHY9E3gOzYzcPkCAmumwvs+3TrN9ZVXdM+U81wAnAQck2aR3Li+9SpKkBWHUl18BqKqfAj8Fjkjyn8ArgQuB3afsujtwRVXdPOCpPgJ8AfglcBXwzQFzJEmSWmXUN0psk+Q9SZ6a5GFJ9gIeR9PQHQU8vXd363ZJXgr8DfeuiRvEN4DrgMOA46pq1Zr+BkmSpDYY9eXX24DtaO5yvZhm3dvxwBFV9WPgBcDzgBXAe3p/xwx6sqoq4Dhg7d6/kiRJC8JIL79W1VXAc+9n+5doHmky0/atpxn7BM3NFzPZEjitqi6bZZmSJEmtN5Y1deOQZENgJ5pn071wwuVIkiSNVWeaOpqHHO8MfKyqTpl0MZIkSePUmaauqvacdA2SJEmT0pmmbmCrVlG3TvcovMGttWiToeYdvNHlQ80DOKmGW6MkSZqskT98WJIkSaNnUydJktQBNnWSJEkdYFMnSZLUATZ1kiRJHdCapi7JIUkum3QdkiRJ81FrmjpJkiTNbChNXZKlSTYaRtYczrlZknXHeU5JkqT5auCmLsniJPskOQG4Enh8b3zDJMuTXJ3k5iRnJFnWd9z+SW5JsneSFUluTXJ6km2m5L8tyZW9fT8FrD+lhGcCV/bOtdugv0OSJKkL5tzUJXlMkiOB/wU+B9wK7AucmSTAKcBWwF8ATwTOBL6VZMu+mAcAhwIHALsCGwEf7jvHC4F/Ag4DngRcBLxlSinHAy8BNgC+keSSJO+a2hxKkiQtBLNq6pJskuQNSc4GzgW2B94EbF5VB1bVmVVVwF7AE4DnV9VZVXVJVb0T+CXw8r7ItYCDe/ucB7wP2CvJ6nreBHyyqo6tqour6nDgrP6aquruqjq1qvYDNgf+uXf+X/RmBw9IMnV2b/XvOSjJ2UnOvrPumM1/AkmSpHlttjN1rweOBlYC21bVc6rq81W1csp+OwEPBK7pXTa9JcktwI7AI/r2W1lVF/V9/w2wNs2MHcAOwA+mZE/9/v+q6uaq+nhV7QU8GXgQ8DHg+TPsv7yqllXVsnVclidJkjpgrVnutxy4C3gFcEGSLwP/AZxWVff07bcIuAp42jQZN/V9vnvKtuo7fs6SPAB4Fs1s4DOBC2hm+04cJE+SJKltZtVEVdVvqurwqnoU8AzgFuCzwBVJjkryxN6uP6a5FLqqd+m1/+/qOdT1M2CXKWO/9z2N3ZMcS3OjxjHAJcBOVfWkqjq6qq6fwzklSZJaa84zY1X1w6p6LbAlzWXZ7YCzkjwN+CbwPeDEJH+eZJskuyZ5d2/7bB0NvDLJgUm2TXIo8JQp+7wM+C9gKbAf8JCqemtVrZjrb5IkSWq72V5+vY/eerovAF9I8iDgnqqqJM+kuXP1IzRr266iafQ+NYfszyV5OHA4zRq9k4D3A/v37XYasEVV3XTfBEmSpIUlzU2rC9eGizetXZY8a6iZV37moUPNO/fJnx1qHsA+D37C0DMlSdJofbO+cE5VLZtum68JkyRJ6gCbOkmSpA6wqZMkSeqABb+mLsk1wK9mseumwLVDPn0bMttQ4ygy21DjKDLbUOMoMttQ4ygy21DjKDLbUOMoMttQ4ygy21DjXDIfVlWbTbdhwTd1s5Xk7JkWJnY5sw01jiKzDTWOIrMNNY4isw01jiKzDTWOIrMNNY4isw01jiKzDTUOK9PLr5IkSR1gUydJktQBNnWzt3yBZrahxlFktqHGUWS2ocZRZLahxlFktqHGUWS2ocZRZLahxlFktqHGoWS6pk6SJKkDnKmTJEnqAJs6SZKkDrCpkyRJ6gCbOkmSpA6wqZMkSeqA/wMYeU+4mCT7VgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_sentence: sit down jake\n",
      "English predict: i am sorry\n",
      "English actual: where are they\n"
     ]
    }
   ],
   "source": [
    "model_attention = model_attention_3\n",
    "index = 12000\n",
    "\n",
    "input_test_sentence = validation[\"question\"].values[index]\n",
    "actual = validation[\"answer_out\"].values[index]\n",
    "input_sentence, pred_string = predict(input_test_sentence)\n",
    "import re\n",
    "print(f\"Input_sentence: {input_sentence}\")\n",
    "print(f\"English predict: {re.sub('<end>', '', pred_string).strip()}\")\n",
    "print(f\"English actual: {re.sub('<end>', '', actual).strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c2841a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_attention_3.save('model_attention_3',save_format='tf')\n",
    "# model_attention_concat.save('model_attention_concat',save_format='tf')\n",
    "# model_attention_dot.save('model_attention_dot',save_format='tf')\n",
    "# loaded_model = tf.keras.models.load_model('model_attention_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e65dbe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_sentence: ma i am gonna stay home and watch jackie gleason\n",
      "English predict: i am sorry i am not\n",
      "English actual: you gonna die without a son\n",
      "================================================================================\n",
      "Input_sentence: this looks like a job for superman \n",
      "English predict: what is it\n",
      "English actual: or batman\n",
      "================================================================================\n",
      "Input_sentence: thanks good night mrs mitchell\n",
      "English predict: i am not going to have a baby\n",
      "English actual: good night john\n",
      "================================================================================\n",
      "Input_sentence: the only answer i have for you\n",
      "English predict: i am not going to hurt you\n",
      "English actual: you passed up a dozen chances to arrest her  what toughened you up\n",
      "================================================================================\n",
      "Input_sentence: lyssa \n",
      "English predict: and i am a little sensitivity\n",
      "English actual: come quickly colwyn i can see the eyes of the beast\n",
      "================================================================================\n",
      "Input_sentence: you must think i am a monster\n",
      "English predict: i am not\n",
      "English actual: no but what a pity life is like that\n",
      "================================================================================\n",
      "Input_sentence: turkey mostly\n",
      "English predict: yes yes yes\n",
      "English actual: turkey is are real smart smarter than most people think\n",
      "================================================================================\n",
      "Input_sentence: it certainly could use a pair of shears and a blue pencil\n",
      "English predict: what is it\n",
      "English actual: i will not have it butchered\n",
      "================================================================================\n",
      "Input_sentence:  you know me\n",
      "English predict: i am not\n",
      "English actual: is this a test sir\n",
      "================================================================================\n",
      "Input_sentence: how can you do it to yourself\n",
      "English predict: i am fine\n",
      "English actual: i do not get you\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "index = 12000\n",
    "model_attention = model_attention_3\n",
    "for index in range(10):\n",
    "    input_test_sentence = train[\"question\"].values[index]\n",
    "    actual = train[\"answer_out\"].values[index]\n",
    "    input_sentence, pred_string = predict(input_test_sentence)\n",
    "    import re\n",
    "    print(f\"Input_sentence: {input_sentence}\")\n",
    "    print(f\"English predict: {re.sub('<end>', '', pred_string).strip()}\")\n",
    "    print(f\"English actual: {re.sub('<end>', '', actual).strip()}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d9228ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_sentence: you are lucky she could not come anyway \n",
      "English predict: i am not\n",
      "English actual: well i am glad you did not thank you\n",
      "================================================================================\n",
      "Input_sentence: i think i have no money\n",
      "English predict: you are not going to be a cop\n",
      "English actual: it is okay  forget it\n",
      "================================================================================\n",
      "Input_sentence: enjoy your last night  where is otis\n",
      "English predict: you are a liar\n",
      "English actual: oh he is coming he got something real special this year\n",
      "================================================================================\n",
      "Input_sentence: what do you want\n",
      "English predict: i am not going to tell you what i am talking about\n",
      "English actual: nothing\n",
      "================================================================================\n",
      "Input_sentence: what is it\n",
      "English predict: i am sorry\n",
      "English actual: i have to pee\n",
      "================================================================================\n",
      "Input_sentence: how you doing man\n",
      "English predict: i am fine\n",
      "English actual: not bad not bad\n",
      "================================================================================\n",
      "Input_sentence: sure why\n",
      "English predict: i am sorry\n",
      "English actual: i am going to the store\n",
      "================================================================================\n",
      "Input_sentence: i hope i am not being crossexamined here\n",
      "English predict: i am sorry i am not\n",
      "English actual: do you feel that way\n",
      "================================================================================\n",
      "Input_sentence: if you push him into something if you ride him\n",
      "English predict: i do not know\n",
      "English actual: you are wrong sean i am where i am today because i was pushed and because i learned to push myself\n",
      "================================================================================\n",
      "Input_sentence: near his right hand about a foot away perhaps more good heavens one does not use a tape measure\n",
      "English predict: yeah i am a little clerk\n",
      "English actual: we do when we get the chance\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_attention = model_attention_3\n",
    "for index in range(10):\n",
    "    input_test_sentence = validation[\"question\"].values[index]\n",
    "    actual = validation[\"answer_out\"].values[index]\n",
    "    input_sentence, pred_string = predict(input_test_sentence)\n",
    "    import re\n",
    "    print(f\"Input_sentence: {input_sentence}\")\n",
    "    print(f\"English predict: {re.sub('<end>', '', pred_string).strip()}\")\n",
    "    print(f\"English actual: {re.sub('<end>', '', actual).strip()}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c9f09",
   "metadata": {},
   "source": [
    "# On random inputs, Sometimes giving Gibberish replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6dcfff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_sentence: hi\n",
      "English predict: what is it\n",
      "================================================================================\n",
      "Input_sentence: where have you been\n",
      "English predict: i am not sure i am sorry i am sorry i am sorry\n",
      "================================================================================\n",
      "Input_sentence: who are you\n",
      "English predict: i am not sure\n",
      "================================================================================\n",
      "Input_sentence: where do you live\n",
      "English predict: i am not sure i am sorry i am not sure i am sorry\n",
      "================================================================================\n",
      "Input_sentence: why are you angry with me did i do anything wrong\n",
      "English predict: i am not sure i am not\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_attention = model_attention_3\n",
    "input_test_sentence = \"hi\"\n",
    "input_sentence, pred_string = predict(input_test_sentence)\n",
    "import re\n",
    "print(f\"Input_sentence: {input_sentence}\")\n",
    "print(f\"English predict: {re.sub('<end>', '', pred_string).strip()}\") \n",
    "print(\"=\"*80)\n",
    "\n",
    "input_test_sentence = \"where have you been\"\n",
    "input_sentence, pred_string = predict(input_test_sentence)\n",
    "import re\n",
    "print(f\"Input_sentence: {input_sentence}\")\n",
    "print(f\"English predict: {re.sub('<end>', '', pred_string).strip()}\") \n",
    "print(\"=\"*80)\n",
    "\n",
    "input_test_sentence = \"who are you\"\n",
    "input_sentence, pred_string = predict(input_test_sentence)\n",
    "import re\n",
    "print(f\"Input_sentence: {input_sentence}\")\n",
    "print(f\"English predict: {re.sub('<end>', '', pred_string).strip()}\") \n",
    "print(\"=\"*80)\n",
    "\n",
    "input_test_sentence = \"where do you live\"\n",
    "input_sentence, pred_string = predict(input_test_sentence)\n",
    "import re\n",
    "print(f\"Input_sentence: {input_sentence}\")\n",
    "print(f\"English predict: {re.sub('<end>', '', pred_string).strip()}\") \n",
    "print(\"=\"*80)\n",
    "\n",
    "input_test_sentence = \"why are you angry with me did i do anything wrong\"\n",
    "input_sentence, pred_string = predict(input_test_sentence)\n",
    "import re\n",
    "print(f\"Input_sentence: {input_sentence}\")\n",
    "print(f\"English predict: {re.sub('<end>', '', pred_string).strip()}\") \n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3fab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         ques_vocab_size = vocab_size_q + 1 #29704\n",
    "#         ans_vocab_size = vocab_size_a + 1 #29595\n",
    "#         encoder_input_length = 27\n",
    "#         dencoder_input_length = 27\n",
    "#         lstm_units = 576\n",
    "#         attention_units = 576\n",
    "#         embedding_dim = 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
