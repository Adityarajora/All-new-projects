{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6da4d51",
   "metadata": {},
   "source": [
    "## In this notebook, I'll try Vanilla encoder-decoder model as a baseline model for my CHATBOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b5112",
   "metadata": {},
   "source": [
    "## Overview\n",
    "***\n",
    "*A chatbot or chatterbot is a software application used to conduct an on-line chat conversation via text or text-to-speech, in lieu of providing direct contact with a live human agent chatbot is a type of software that can help human by automating conversations and interact with them through messaging platforms. here are different approaches and tools that you can use when building chatbots. Depending on the use case you want to address, some technologies are more appropriate than others. Combining artificial intelligence forms such as natural language processing, machine learning, and semantic understanding may be the best option to achieve the desired results.*\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bcc417",
   "metadata": {},
   "source": [
    "## How to build a Chatbot for our task?\n",
    "***\n",
    "ChatBots are usually Task specific means if there a chatbot which serves only food delivery app have trained on a dataset which\n",
    "completely different from the dataset on which chatbot which serves online healthcare app. Similary, for this kaggle problem\n",
    "we have provided with movie dataset which may feel that its not specific to any task, but actually it is specific to how people\n",
    "will interect generally as these movie dialogues are nothing but daily life conversation between people however, that chatbot\n",
    "may reply things which sounds too much dramatic and filmy like some dialogue of Tom cruise, shah rukh khan etc.\n",
    "\n",
    "We can approch this problem by applying Neural network models like encoder-decoder architecture with some attention mechanism.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2e9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import ast\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff9639b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224c491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dedd8961",
   "metadata": {},
   "source": [
    "## Loading data, prepared while EDA and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ab6cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = joblib.load(\"train\")\n",
    "validation = joblib.load(\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61ccbd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_in</th>\n",
       "      <th>answer_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31606</th>\n",
       "      <td>ma i am gonna stay home and watch jackie gleason</td>\n",
       "      <td>&lt;start&gt; you gonna die without a son &lt;end&gt;</td>\n",
       "      <td>you gonna die without a son &lt;end&gt; &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34339</th>\n",
       "      <td>this looks like a job for superman</td>\n",
       "      <td>&lt;start&gt; or batman</td>\n",
       "      <td>or batman  &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32082</th>\n",
       "      <td>thanks good night mrs mitchell</td>\n",
       "      <td>&lt;start&gt; good night john</td>\n",
       "      <td>good night john &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109702</th>\n",
       "      <td>the only answer i have for you</td>\n",
       "      <td>&lt;start&gt; you passed up a dozen chances to arres...</td>\n",
       "      <td>you passed up a dozen chances to arrest her  w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26991</th>\n",
       "      <td>lyssa</td>\n",
       "      <td>&lt;start&gt; come quickly colwyn i can see the eyes...</td>\n",
       "      <td>come quickly colwyn i can see the eyes of the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "31606   ma i am gonna stay home and watch jackie gleason   \n",
       "34339                this looks like a job for superman    \n",
       "32082                     thanks good night mrs mitchell   \n",
       "109702                    the only answer i have for you   \n",
       "26991                                             lyssa    \n",
       "\n",
       "                                                answer_in  \\\n",
       "31606           <start> you gonna die without a son <end>   \n",
       "34339                                  <start> or batman    \n",
       "32082                             <start> good night john   \n",
       "109702  <start> you passed up a dozen chances to arres...   \n",
       "26991   <start> come quickly colwyn i can see the eyes...   \n",
       "\n",
       "                                               answer_out  \n",
       "31606             you gonna die without a son <end> <end>  \n",
       "34339                                    or batman  <end>  \n",
       "32082                               good night john <end>  \n",
       "109702  you passed up a dozen chances to arrest her  w...  \n",
       "26991   come quickly colwyn i can see the eyes of the ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cf89c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_in</th>\n",
       "      <th>answer_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38803</th>\n",
       "      <td>you are lucky she could not come anyway</td>\n",
       "      <td>&lt;start&gt; well i am glad you did not thank you</td>\n",
       "      <td>well i am glad you did not thank you &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>i think i have no money</td>\n",
       "      <td>&lt;start&gt; it is okay  forget it</td>\n",
       "      <td>it is okay  forget it &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21215</th>\n",
       "      <td>enjoy your last night  where is otis</td>\n",
       "      <td>&lt;start&gt; oh he is coming he got something real ...</td>\n",
       "      <td>oh he is coming he got something real special ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23141</th>\n",
       "      <td>what do you want</td>\n",
       "      <td>&lt;start&gt; nothing</td>\n",
       "      <td>nothing &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18195</th>\n",
       "      <td>what is it</td>\n",
       "      <td>&lt;start&gt; i have to pee</td>\n",
       "      <td>i have to pee &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question  \\\n",
       "38803  you are lucky she could not come anyway    \n",
       "7194                    i think i have no money   \n",
       "21215      enjoy your last night  where is otis   \n",
       "23141                          what do you want   \n",
       "18195                                what is it   \n",
       "\n",
       "                                               answer_in  \\\n",
       "38803       <start> well i am glad you did not thank you   \n",
       "7194                       <start> it is okay  forget it   \n",
       "21215  <start> oh he is coming he got something real ...   \n",
       "23141                                    <start> nothing   \n",
       "18195                              <start> i have to pee   \n",
       "\n",
       "                                              answer_out  \n",
       "38803         well i am glad you did not thank you <end>  \n",
       "7194                         it is okay  forget it <end>  \n",
       "21215  oh he is coming he got something real special ...  \n",
       "23141                                      nothing <end>  \n",
       "18195                                i have to pee <end>  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f78a41",
   "metadata": {},
   "source": [
    "####  \\<start>, \\<end> tokens, for to apply one time-shifted technique!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d1df2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tknizer_q = Tokenizer(filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "tknizer_q.fit_on_texts(train['question'].values)\n",
    "tknizer_a = Tokenizer(filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "tknizer_a.fit_on_texts(train['answer_in'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4627ba4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29704\n",
      "29595\n"
     ]
    }
   ],
   "source": [
    "q_word_idx = tknizer_q.word_index\n",
    "q_idx_word = {v: k for k, v in q_word_idx.items()}\n",
    "\n",
    "a_word_idx = tknizer_a.word_index\n",
    "a_idx_word = {v: k for k, v in a_word_idx.items()}\n",
    "\n",
    "print(len(q_word_idx.keys()))\n",
    "print(len(a_word_idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd5c82ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<start> you gonna die without a son <end>', '<start> or batman ',\n",
       "       '<start> good night john', ...,\n",
       "       '<start> i have retrieved certain pieces of information on miss katarina stratford i think you will find helpful',\n",
       "       '<start> now you',\n",
       "       '<start> all kinds of shit would come out like the ellsberg thing  you knew about that henry did not you'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['answer_in'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04070324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14863)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_word_idx['<start>'], a_word_idx['<end>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f319c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_a = len(a_word_idx.keys())\n",
    "vocab_size_q = len(q_word_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fafafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "808e7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data, tknizer_q, tknizer_a):\n",
    "        self.encoder_inps = data['question'].values\n",
    "        self.decoder_inps = data['answer_in'].values\n",
    "        self.decoder_outs = data['answer_out'].values\n",
    "        self.tknizer_q = tknizer_q\n",
    "        self.tknizer_a = tknizer_a\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        self.encoder_seq = self.tknizer_q.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
    "        self.decoder_inp_seq = self.tknizer_a.texts_to_sequences([self.decoder_inps[i]])\n",
    "        self.decoder_out_seq = self.tknizer_a.texts_to_sequences([self.decoder_outs[i]])\n",
    "\n",
    "        self.encoder_seq = pad_sequences(self.encoder_seq,maxlen=27, dtype='int32', padding='post')\n",
    "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq,maxlen=27, dtype='int32', padding='post')\n",
    "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq,maxlen=27, dtype='int32', padding='post')\n",
    "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
    "\n",
    "    def __len__(self): # your model.fit_gen requires this function\n",
    "        return len(self.encoder_inps)\n",
    "\n",
    "    \n",
    "class Dataloder(tf.keras.utils.Sequence):    \n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "\n",
    "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
    "        return tuple([[batch[0],batch[1]],batch[2]])\n",
    "\n",
    "    def __len__(self):  # your model.fit_gen requires this function\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3403777d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 27) (128, 27) (128, 27)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(train, tknizer_q, tknizer_a)\n",
    "test_dataset  = Dataset(validation, tknizer_q, tknizer_a)\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=128)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=128)\n",
    "\n",
    "print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211926d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ([q,a_in],a_out), ([q,a_in],a_out), ([q,a_in],a_out) ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5726d8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "599aea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "    def __init__(self, ques_vocab_size, embedding_dim, encoder_input_length, lstm_units):\n",
    "\n",
    "        #Initialize Embedding layer\n",
    "        #Intialize Encoder LSTM layer\n",
    "        super().__init__(name=\"encode_model\")\n",
    "        self.ques_vocab_size = ques_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.encoder_input_length = encoder_input_length\n",
    "        self.embedding = Embedding(input_dim=self.ques_vocab_size, output_dim=self.embedding_dim, input_length=self.encoder_input_length,\\\n",
    "                  mask_zero=True, name=\"embedding_layer_encoder\")\n",
    "        self.LSTM = LSTM(self.lstm_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
    "\n",
    "    def call(self,input_sequence):\n",
    "        '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "          returns -- All encoder_outputs, last time steps hidden and cell state\n",
    "        '''\n",
    "        input_embeddings = self.embedding(input_sequence)\n",
    "        self.lstm_output, self.lstm_state_h, self.lstm_state_c = self.LSTM(input_embeddings)\n",
    "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
    "\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "        '''\n",
    "        Given a batch size it will return intial hidden state and intial cell state.\n",
    "        If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "        '''\n",
    "        lstm_state_h = tf.zeros([batch_size, self.lstm_size])\n",
    "        lstm_state_c = tf.zeros([batch_size, self.lstm_size])\n",
    "\n",
    "        return lstm_state_h, lstm_state_c\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78e820da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, ans_vocab_size, embedding_dim, decoder_input_length, lstm_units):\n",
    "        #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "        super().__init__(name=\"Decode_Attention\")\n",
    "        self.ans_vocab_size = ans_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.decoder_input_length = decoder_input_length\n",
    "        self.lstm_units = lstm_units \n",
    "        self.embedding = Embedding(input_dim=self.ans_vocab_size, output_dim=self.embedding_dim,input_length=self.decoder_input_length,\\\n",
    "                  mask_zero=True, trainable=True, name=\"embedding_layer_decoder\")\n",
    "        self.LSTM = LSTM(self.lstm_units, return_sequences=True, return_state=True, name=\"Dencoder_LSTM\")\n",
    "        \n",
    " \n",
    "    def call(self,input_sequence,initial_states):\n",
    "        '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to decoder_lstm\n",
    "        \n",
    "          returns -- decoder_output,decoder_final_state_h,decoder_final_state_c\n",
    "        '''\n",
    "        state_h, state_c = initial_states[0], initial_states[1]\n",
    "        input_embeddings = self.embedding(input_sequence)\n",
    "        lstm_output, _, _  = self.LSTM(input_embeddings, initial_state=[state_h, state_c])\n",
    "        return lstm_output, _, _\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d387f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "    def __init__(self,ques_vocab_size, ans_vocab_size, encoder_input_length, dencoder_input_length, emb_dim, lstm_units):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super().__init__(name=\"Encoder_Decoder_model\")\n",
    "    \n",
    "        self.encoder = Encoder(ques_vocab_size, emb_dim, encoder_input_length, lstm_units )\n",
    "        self.decoder = Decoder(ans_vocab_size, emb_dim, dencoder_input_length, lstm_units)\n",
    "        self.dense   = Dense(ans_vocab_size, activation='softmax')\n",
    "                             \n",
    "    def call(self,data):\n",
    "        #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "        # Decoder initial states are encoder final states, Initialize it accordingly\n",
    "        # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
    "        # return the decoder output\n",
    "        input_q, input_a = data[0], data[1]\n",
    "        encoder_output, encoder_h, encoder_c = self.encoder(input_q)\n",
    "        decoder_output, _, _ = self.decoder(input_a, [encoder_h, encoder_c])\n",
    "        output = self.dense(decoder_output)\n",
    "        return output\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1025facd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.8447 - accuracy: 0.1668\n",
      "Epoch 1: val_loss improved from inf to 1.58777, saving model to model_save_vanilla_3\\weights-01-1.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_vanilla_3\\weights-01-1.59\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_vanilla_3\\weights-01-1.59\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000276895DBE80> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000276895D7670> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 162s 180ms/step - loss: 1.8447 - accuracy: 0.1668 - val_loss: 1.5878 - val_accuracy: 0.2136\n",
      "Epoch 2/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.6064 - accuracy: 0.2209\n",
      "Epoch 2: val_loss improved from 1.58777 to 1.52136, saving model to model_save_vanilla_3\\weights-02-1.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_vanilla_3\\weights-02-1.52\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_vanilla_3\\weights-02-1.52\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000276895DBE80> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000276895D7670> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 156s 180ms/step - loss: 1.6064 - accuracy: 0.2209 - val_loss: 1.5214 - val_accuracy: 0.2283\n",
      "Epoch 3/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.5315 - accuracy: 0.2332\n",
      "Epoch 3: val_loss improved from 1.52136 to 1.49262, saving model to model_save_vanilla_3\\weights-03-1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_vanilla_3\\weights-03-1.49\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_vanilla_3\\weights-03-1.49\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000276895DBE80> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000276895D7670> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 159s 183ms/step - loss: 1.5315 - accuracy: 0.2332 - val_loss: 1.4926 - val_accuracy: 0.2357\n",
      "Epoch 4/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.4773 - accuracy: 0.2420\n",
      "Epoch 4: val_loss improved from 1.49262 to 1.47894, saving model to model_save_vanilla_3\\weights-04-1.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_vanilla_3\\weights-04-1.48\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_vanilla_3\\weights-04-1.48\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000276895DBE80> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000276895D7670> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 160s 184ms/step - loss: 1.4773 - accuracy: 0.2420 - val_loss: 1.4789 - val_accuracy: 0.2409\n",
      "Epoch 5/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.4308 - accuracy: 0.2491\n",
      "Epoch 5: val_loss improved from 1.47894 to 1.47327, saving model to model_save_vanilla_3\\weights-05-1.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_vanilla_3\\weights-05-1.47\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_save_vanilla_3\\weights-05-1.47\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000276895DBE80> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x00000276895D7670> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871/871 [==============================] - 160s 183ms/step - loss: 1.4308 - accuracy: 0.2491 - val_loss: 1.4733 - val_accuracy: 0.2451\n",
      "Epoch 6/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.3891 - accuracy: 0.2551\n",
      "Epoch 6: val_loss did not improve from 1.47327\n",
      "871/871 [==============================] - 142s 163ms/step - loss: 1.3891 - accuracy: 0.2551 - val_loss: 1.4748 - val_accuracy: 0.2471\n",
      "Epoch 7/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.3491 - accuracy: 0.2608\n",
      "Epoch 7: val_loss did not improve from 1.47327\n",
      "871/871 [==============================] - 138s 159ms/step - loss: 1.3491 - accuracy: 0.2608 - val_loss: 1.4788 - val_accuracy: 0.2481\n",
      "Epoch 8/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.3099 - accuracy: 0.2672\n",
      "Epoch 8: val_loss did not improve from 1.47327\n",
      "871/871 [==============================] - 145s 166ms/step - loss: 1.3099 - accuracy: 0.2672 - val_loss: 1.4863 - val_accuracy: 0.2472\n",
      "Epoch 9/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.2734 - accuracy: 0.2741\n",
      "Epoch 9: val_loss did not improve from 1.47327\n",
      "871/871 [==============================] - 146s 168ms/step - loss: 1.2734 - accuracy: 0.2741 - val_loss: 1.4981 - val_accuracy: 0.2478\n",
      "Epoch 10/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.2376 - accuracy: 0.2832\n",
      "Epoch 10: val_loss did not improve from 1.47327\n",
      "871/871 [==============================] - 146s 167ms/step - loss: 1.2376 - accuracy: 0.2832 - val_loss: 1.5104 - val_accuracy: 0.2471\n",
      "Epoch 11/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.2038 - accuracy: 0.2932\n",
      "Epoch 11: val_loss did not improve from 1.47327\n",
      "871/871 [==============================] - 146s 168ms/step - loss: 1.2038 - accuracy: 0.2932 - val_loss: 1.5232 - val_accuracy: 0.2467\n",
      "Epoch 12/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.1727 - accuracy: 0.3032\n",
      "Epoch 12: val_loss did not improve from 1.47327\n",
      "871/871 [==============================] - 147s 169ms/step - loss: 1.1727 - accuracy: 0.3032 - val_loss: 1.5382 - val_accuracy: 0.2472\n",
      "Epoch 13/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.1436 - accuracy: 0.3128\n",
      "Epoch 13: val_loss did not improve from 1.47327\n",
      "871/871 [==============================] - 146s 168ms/step - loss: 1.1436 - accuracy: 0.3128 - val_loss: 1.5541 - val_accuracy: 0.2448\n",
      "Epoch 14/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.1160 - accuracy: 0.3231\n",
      "Epoch 14: val_loss did not improve from 1.47327\n",
      "871/871 [==============================] - 138s 158ms/step - loss: 1.1160 - accuracy: 0.3231 - val_loss: 1.5692 - val_accuracy: 0.2436\n",
      "Epoch 15/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.0900 - accuracy: 0.3331\n",
      "Epoch 15: val_loss did not improve from 1.47327\n",
      "871/871 [==============================] - 137s 158ms/step - loss: 1.0900 - accuracy: 0.3331 - val_loss: 1.5849 - val_accuracy: 0.2417\n",
      "Epoch 16/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.0649 - accuracy: 0.3428\n",
      "Epoch 16: val_loss did not improve from 1.47327\n",
      "871/871 [==============================] - 137s 157ms/step - loss: 1.0649 - accuracy: 0.3428 - val_loss: 1.6005 - val_accuracy: 0.2434\n",
      "Epoch 17/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.0413 - accuracy: 0.3528\n",
      "Epoch 17: val_loss did not improve from 1.47327\n",
      "871/871 [==============================] - 137s 157ms/step - loss: 1.0413 - accuracy: 0.3528 - val_loss: 1.6168 - val_accuracy: 0.2395\n",
      "Epoch 18/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 1.0177 - accuracy: 0.3626\n",
      "Epoch 18: val_loss did not improve from 1.47327\n",
      "871/871 [==============================] - 137s 157ms/step - loss: 1.0177 - accuracy: 0.3626 - val_loss: 1.6327 - val_accuracy: 0.2378\n",
      "Epoch 19/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 0.9950 - accuracy: 0.3728\n",
      "Epoch 19: val_loss did not improve from 1.47327\n",
      "871/871 [==============================] - 137s 157ms/step - loss: 0.9950 - accuracy: 0.3728 - val_loss: 1.6499 - val_accuracy: 0.2369\n",
      "Epoch 20/20\n",
      "871/871 [==============================] - ETA: 0s - loss: 0.9723 - accuracy: 0.3831\n",
      "Epoch 20: val_loss did not improve from 1.47327\n",
      "871/871 [==============================] - 137s 157ms/step - loss: 0.9723 - accuracy: 0.3831 - val_loss: 1.6654 - val_accuracy: 0.2356\n"
     ]
    }
   ],
   "source": [
    "ques_vocab_size = vocab_size_q + 1\n",
    "ans_vocab_size = vocab_size_a + 1\n",
    "encoder_input_length = 27\n",
    "dencoder_input_length = 27\n",
    "lstm_units = 512\n",
    "emb_dim = 100\n",
    "\n",
    "model_vanilla = encoder_decoder(ques_vocab_size, \n",
    "                                  ans_vocab_size, \n",
    "                                  encoder_input_length, \n",
    "                                  dencoder_input_length,\n",
    "                                  emb_dim,\n",
    "                                  lstm_units)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model_vanilla.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "train_steps=train.shape[0]//128\n",
    "valid_steps=validation.shape[0]//128\n",
    "history_dot = model_vanilla.fit(train_dataloader, \\\n",
    "                                      steps_per_epoch=train_steps, \\\n",
    "                                      epochs=20, \\\n",
    "                                      validation_data=test_dataloader, \\\n",
    "                                      validation_steps=valid_steps,\n",
    "                                      callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab8f74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, TensorBoard, LearningRateScheduler\n",
    "from sklearn.metrics import recall_score, f1_score, roc_curve, auc\n",
    "import datetime\n",
    "\n",
    "filepath=\"model_save_vanilla_3/weights-{epoch:02d}-{val_loss:.2f}\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', save_format=\"tf\", save_freq=\"epoch\",  verbose=1, save_best_only=True, mode='auto')\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95bf8f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2783bf2c7f0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history_dot.history['loss']\n",
    "val_loss = history_dot.history['val_loss']\n",
    "epoch = 20\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss/val_loss')\n",
    "plt.title('Learning')\n",
    "plt.plot(range(epoch), loss, label = \"loss\")\n",
    "plt.plot(range(epoch), val_loss, label = \"val_loss\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0f8f451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder_Decoder_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encode_model (Encoder)      multiple                  4225924   \n",
      "                                                                 \n",
      " Decode_Attention (Decoder)  multiple                  4215024   \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  15182748  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,623,696\n",
      "Trainable params: 23,623,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_vanilla.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d285fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    sentence = sentence.split(\" \")\n",
    "    predicted_sentence = predicted_sentence.split(\" \")\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "50905e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_test_sentence):\n",
    "\n",
    "    '''\n",
    "    A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
    "    B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "    C. Initialize index of <start> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
    "    D. till we reach max_length of decoder or till the model predicted word <end>:\n",
    "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
    "         Save the attention weights\n",
    "         And get the word using the tokenizer(word index) and then store it in a string.\n",
    "    E. Call plot_attention(#params)\n",
    "    F. Return the predicted sentence\n",
    "    '''\n",
    "    ENCODER_SEQ_LEN = 27\n",
    "    DECODER_SEQ_LEN = 27 \n",
    "    nums = tknizer_q.texts_to_sequences([input_test_sentence])\n",
    "    nums_padded = pad_sequences(nums, maxlen=27, dtype='int32', padding='post')\n",
    "    encoder_output, enc_state_h, enc_state_c = model_vanilla.layers[0](nums_padded)\n",
    "    pred, alphas = [], []\n",
    "    cur_vec = np.zeros((1, 1))+1\n",
    "    states_values = [enc_state_h, enc_state_c]\n",
    "    for i in range(DECODER_SEQ_LEN):\n",
    "        cur_emb = model_vanilla.layers[1].embedding(cur_vec)\n",
    "        output, state_h, state_c = model_vanilla.layers[1].LSTM(cur_emb, initial_state=states_values) \n",
    "        infe_output = model_vanilla.layers[2](output)\n",
    "        states_values = [state_h, state_c]  \n",
    "         \n",
    "        cur_vec = np.reshape(np.argmax(output), (1, 1)) \n",
    "        print(f\"at time step {i} the word is \", cur_vec)\n",
    "        \n",
    "        \n",
    "        if a_idx_word[cur_vec[0][0]] == '<end>':\n",
    "            break\n",
    "        pred.append(cur_vec)\n",
    "        \n",
    "        \n",
    "     \n",
    "    pred_string = \"\"\n",
    "\n",
    "    pred_string = \" \".join([a_idx_word[i[0][0]] for i in pred]) + \" <end>\"\n",
    "    \n",
    "    print(\"PREDICTED STRING:\",pred_string)\n",
    "\n",
    "    \n",
    "    return  input_test_sentence, pred_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d4cfd97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29595"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(a_idx_word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6938fd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29595"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a_idx_word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaea40d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "95521726",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at time step 0 the word is  [[253]]\n",
      "at time step 1 the word is  [[455]]\n",
      "at time step 2 the word is  [[7]]\n",
      "at time step 3 the word is  [[73]]\n",
      "at time step 4 the word is  [[80]]\n",
      "at time step 5 the word is  [[40]]\n",
      "at time step 6 the word is  [[111]]\n",
      "at time step 7 the word is  [[28]]\n",
      "at time step 8 the word is  [[297]]\n",
      "at time step 9 the word is  [[448]]\n",
      "at time step 10 the word is  [[344]]\n",
      "at time step 11 the word is  [[78]]\n",
      "at time step 12 the word is  [[496]]\n",
      "at time step 13 the word is  [[93]]\n",
      "at time step 14 the word is  [[85]]\n",
      "at time step 15 the word is  [[93]]\n",
      "at time step 16 the word is  [[496]]\n",
      "at time step 17 the word is  [[389]]\n",
      "at time step 18 the word is  [[267]]\n",
      "at time step 19 the word is  [[297]]\n",
      "at time step 20 the word is  [[31]]\n",
      "at time step 21 the word is  [[77]]\n",
      "at time step 22 the word is  [[125]]\n",
      "at time step 23 the word is  [[78]]\n",
      "at time step 24 the word is  [[496]]\n",
      "at time step 25 the word is  [[284]]\n",
      "at time step 26 the word is  [[161]]\n",
      "PREDICTED STRING: knew dear to tell as get gonna your wanna walk men were running never look never running beautiful best wanna can time anything were running used hey <end>\n",
      "Input_sentence: what do you read what is the last book you read\n",
      "English predict: knew dear to tell as get gonna your wanna walk men were running never look never running beautiful best wanna can time anything were running used hey <end>\n",
      "English actual: lust for life it is the story of vincent van gough\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "input_test_sentence = validation[\"question\"].values[index]\n",
    "actual = validation[\"answer_out\"].values[index]\n",
    "input_sentence, pred_string = predict(input_test_sentence)\n",
    "import re\n",
    "print(f\"Input_sentence: {input_sentence}\")\n",
    "print(f\"English predict: {pred_string}\")\n",
    "print(f\"English actual: {re.sub('<end>', '', actual).strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036a574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "be7515eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at time step 0 the word is  [[338]]\n",
      "at time step 1 the word is  [[139]]\n",
      "at time step 2 the word is  [[296]]\n",
      "at time step 3 the word is  [[37]]\n",
      "at time step 4 the word is  [[496]]\n",
      "at time step 5 the word is  [[152]]\n",
      "at time step 6 the word is  [[136]]\n",
      "at time step 7 the word is  [[7]]\n",
      "at time step 8 the word is  [[16]]\n",
      "at time step 9 the word is  [[184]]\n",
      "at time step 10 the word is  [[184]]\n",
      "at time step 11 the word is  [[144]]\n",
      "at time step 12 the word is  [[119]]\n",
      "at time step 13 the word is  [[7]]\n",
      "at time step 14 the word is  [[31]]\n",
      "at time step 15 the word is  [[77]]\n",
      "at time step 16 the word is  [[313]]\n",
      "at time step 17 the word is  [[318]]\n",
      "at time step 18 the word is  [[43]]\n",
      "at time step 19 the word is  [[49]]\n",
      "at time step 20 the word is  [[371]]\n",
      "at time step 21 the word is  [[371]]\n",
      "at time step 22 the word is  [[139]]\n",
      "at time step 23 the word is  [[161]]\n",
      "at time step 24 the word is  [[37]]\n",
      "at time step 25 the word is  [[55]]\n",
      "at time step 26 the word is  [[68]]\n",
      "PREDICTED STRING: die find world they running guess thank to of into into because talk to can time hundred woman there him mine mine find hey they if see <end>\n",
      "Input_sentence: i do not know just some messy little scrap you know that bullshit\n",
      "English predict: die find world they running guess thank to of into into because talk to can time hundred woman there him mine mine find hey they if see\n",
      "English actual: do you know where he went\n"
     ]
    }
   ],
   "source": [
    "index = 450\n",
    "\n",
    "input_test_sentence = validation[\"question\"].values[index]\n",
    "actual = validation[\"answer_out\"].values[index]\n",
    "input_sentence, pred_string = predict(input_test_sentence)\n",
    "import re\n",
    "print(f\"Input_sentence: {input_sentence}\")\n",
    "print(f\"English predict: {re.sub('<end>', '', pred_string).strip()}\")\n",
    "print(f\"English actual: {re.sub('<end>', '', actual).strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf7993",
   "metadata": {},
   "source": [
    "## Completely Gibberish results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f61404",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ques_vocab_size = vocab_size_q + 1 #29704\n",
    "    ans_vocab_size = vocab_size_a + 1 #29595\n",
    "    encoder_input_length = 27\n",
    "    dencoder_input_length = 27\n",
    "    lstm_units = 512\n",
    "    embedding_dim = 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
